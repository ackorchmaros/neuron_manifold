################################ Install python entropy toolbox ################################
# author:Annachiara Korchmaros

#download
git clone https://github.com/gregversteeg/NPEET.git

#set directory
mv Desktop/NPEET Desktop/neuron_manifold/fig11_S8

# test package
cd  Desktop/dimensionality/scripts/NPEET/npeet
python3

## example 1
import entropy_estimators as ee
x = [[1.3],[3.7],[5.1],[2.4],[3.4]]
y = [[1.5],[3.32],[5.3],[2.3],[3.3]]

print ee.cmi(x, y, z)

## DISCRETE ESTIMATOR
# inside python test.py
# input is a list of lists
from npeet import entropy_estimators as ee
from math import log, pi
import numpy as np
import numpy.random as nr
import random
from numpy.linalg import det

print("\n\nTest of the discrete entropy estimators\n")
print("For z = y xor x, w/x, y uniform random binary, we should get H(x)=H(y)=H(z) = 1, H(x:y) etc = 0, H(x:y|z) = 1")
x = [0, 0, 0, 0, 1, 1, 1, 1]
y = [0, 1, 0, 1, 0, 1, 0, 1]
z = [0, 1, 0, 1, 1, 0, 1, 0]
print("H(x), H(y), H(z)", ee.entropyd(x), ee.entropyd(y), ee.entropyd(z))
print("H(x:y), etc", ee.midd(x, y), ee.midd(z, y), ee.midd(x, z))
print("H(x:y|z), etc", ee.cmidd(x, y, z), ee.cmidd(z, y, x), ee.cmidd(x, z, y))

## Differential entropy estimator
# input is a list of lists
# inside python test.py
from npeet import entropy_estimators as ee
from math import log, pi
import numpy as np
import numpy.random as nr
import random
from numpy.linalg import det

print("For a uniform distribution with width alpha, the differential entropy is log_2 alpha, setting alpha = 2")
print("and using k=1, 2, 3, 4, 5")
print("result:", [ee.entropy([[2 * random.random()] for i in range(1000)], k=j + 1) for j in range(5)]